# -*- coding: utf-8 -*-
"""AudioGeneration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TwQcZ69tGjtRVXSdr71WmdCrGnBuP7TU

###STANDARD VAE Implementation using MSE as reconstruction error and with a latent dimension of 128.
"""

import os
import pickle

from tensorflow.keras import Model
from tensorflow.keras.layers import (
    Input, Conv2D, ReLU, BatchNormalization, Flatten,
    Dense, Reshape, Conv2DTranspose, Activation, Lambda
)
from tensorflow.keras import backend as K
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import MeanSquaredError
import numpy as np
import tensorflow as tf

tf.compat.v1.disable_eager_execution()

def build_encoder(input_shape, conv_filters, conv_kernels, conv_strides, latent_space_dim):
    encoder_input = Input(shape=input_shape, name="encoder_input")
    x = encoder_input
    for i, (filters, kernels, strides) in enumerate(zip(conv_filters, conv_kernels, conv_strides)):
        x = Conv2D(
            filters=filters,
            kernel_size=kernels,
            strides=strides,
            padding="same",
            name=f"encoder_conv_layer_{i + 1}"
        )(x)
        x = ReLU(name=f"encoder_relu_{i + 1}")(x)
        x = BatchNormalization(name=f"encoder_bn_{i + 1}")(x)

    shape_before_bottleneck = K.int_shape(x)[1:]
    x = Flatten()(x)
    mu = Dense(latent_space_dim, name="mu")(x)
    log_variance = Dense(latent_space_dim, name="log_variance")(x)

    def sample_point_from_normal_distribution(args):
        mu, log_variance = args
        epsilon = K.random_normal(shape=K.shape(mu), mean=0., stddev=1.)
        return mu + K.exp(log_variance / 2) * epsilon

    encoder_output = Lambda(
        sample_point_from_normal_distribution,
        name="encoder_output"
    )([mu, log_variance])

    encoder = Model(encoder_input, encoder_output, name="encoder")
    encoder_mu = Model(encoder_input, mu, name="encoder_mu")  # New model to output just mu values

    return encoder, encoder_mu, shape_before_bottleneck, mu, log_variance


def build_decoder(
    latent_space_dim, shape_before_bottleneck, conv_filters, conv_kernels, conv_strides
):
    decoder_input = Input(shape=latent_space_dim, name="decoder_input")
    num_neurons = np.prod(shape_before_bottleneck)
    x = Dense(num_neurons, name="decoder_dense")(decoder_input)
    x = Reshape(shape_before_bottleneck)(x)

    for i, (filters, kernels, strides) in reversed(list(enumerate(zip(conv_filters, conv_kernels, conv_strides)))):
        x = Conv2DTranspose(
            filters=filters,
            kernel_size=kernels,
            strides=strides,
            padding="same"
        )(x)
        x = ReLU(name=f"decoder_relu_{i + 1}")(x)
        x = BatchNormalization(name=f"decoder_bn_{i + 1}")(x)

    decoder_output = Conv2DTranspose(
        filters=1,
        kernel_size=conv_kernels[0],
        strides=(1, 1),
        padding="same",
        name=f"decoder_conv_transpose_layer_{len(conv_filters)}"
    )(x)

    output_layer = Activation("sigmoid", name="sigmoid_layer")(decoder_output)

    return Model(decoder_input, output_layer, name="decoder")


def build_autoencoder(encoder, decoder):
    model_input = encoder.input
    model_output = decoder(encoder(model_input))
    return Model(model_input, model_output, name="autoencoder")


def combined_loss(y_target, y_predicted, mu, log_variance, reconstruction_loss_weight):
    reconstruction_loss = K.mean(K.square(y_target - y_predicted), axis=[1, 2, 3])
    kl_loss = -0.5 * K.sum(1 + log_variance - K.square(mu) - K.exp(log_variance), axis=1)
    return reconstruction_loss_weight * reconstruction_loss + kl_loss

def save_autoencoder_weights(autoencoder, folder):
    if not os.path.exists(folder):
        os.makedirs(folder)

    weights_path = os.path.join(folder, "weights.h5")
    autoencoder.save_weights(weights_path)

def load_autoencoder(autoencoder, folder):
    weights_path = os.path.join(folder, "weights.h5")
    autoencoder.load_weights(weights_path)

    return autoencoder

if __name__ == "__main__":
    input_shape = (256, 64, 1)
    conv_filters = (512, 256, 128, 64, 32)
    conv_kernels = (3, 3, 3, 3, 3)
    conv_strides = (2, 2, 2, 2, (2, 1))
    latent_space_dim = 128
    reconstruction_loss_weight = 1000000

    encoder, encoder_mu, shape_before_bottleneck, mu, log_variance = build_encoder(
        input_shape, conv_filters, conv_kernels, conv_strides, latent_space_dim
    )

    decoder = build_decoder(
        latent_space_dim, shape_before_bottleneck, conv_filters, conv_kernels, conv_strides
    )

    autoencoder = build_autoencoder(encoder, decoder)

    optimizer = tf.keras.optimizers.legacy.Adam(0.0005)
    autoencoder.compile(
        optimizer=optimizer,
        loss=lambda y_true, y_pred: combined_loss(y_true, y_pred, mu, log_variance, reconstruction_loss_weight),
        metrics=[MeanSquaredError()],
    )

    autoencoder.summary()

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/My Drive/AudioGeneration"

import os
import numpy as np

def get_dict(directory_path):
  loaded_dict = {}

  file_list = os.listdir(directory_path)

  for file_name in file_list:
      if file_name.endswith('.npy'):
          file_path = os.path.join(directory_path, file_name)
          array = np.load(file_path)
          array_with_new_axis = np.expand_dims(array, axis=-1)
          loaded_dict[file_name] = array_with_new_axis
  return loaded_dict

loaded_dict = get_dict("/content/drive/My Drive/AudioGeneration/spectrograms")

# to load models weight
# autoencoder = load_autoencoder(autoencoder, 'model')

keys = list(loaded_dict.keys())
train_d = np.array([loaded_dict[key] for key in keys])

history = autoencoder.fit(
    train_d,
    train_d,
    batch_size=64,
    epochs=50,
    shuffle=True
)

import librosa
import soundfile as sf
import numpy as np
import matplotlib.pyplot as plt
import librosa.display
import tensorflow as tf
from IPython.display import Audio, display


def inverse_normalize_spectrogram(normalized_spectrogram, min_val, max_val):
    return normalized_spectrogram * (max_val - min_val) + min_val

def reconstruct_audio(stft, output_path, min_val, max_val):
    stft = stft[:, :, 0]
    magnitude_db = inverse_normalize_spectrogram(stft, min_val, max_val)
    magnitude_linear = librosa.db_to_amplitude(magnitude_db)
    y_reconstructed = librosa.istft(magnitude_linear, hop_length=256)
    if not os.path.exists('samples'):
        os.makedirs('samples')
    file_path = os.path.join('samples', output_path)
    current_max = np.max(np.abs(y_reconstructed))
    desired_max = 0.9

    if current_max == 0:
        amplification_factor = 1
    else:
        amplification_factor = desired_max / current_max

    amplified_audio = y_reconstructed * amplification_factor
    amplified_audio = np.clip(amplified_audio, -1, 1)

    sf.write(file_path, amplified_audio, 22050)

    return magnitude_db

import numpy as np
import matplotlib.pyplot as plt
import soundfile as sf
import os

def reconstruct(encoder, decoder, batch_of_images):
    latent_representations = encoder.predict(batch_of_images)
    reconstructed_images = decoder.predict(latent_representations)
    return reconstructed_images, latent_representations

def get_test_batch(n_reconstructions, l_dict):
    keys = list(l_dict.keys())
    random_keys = np.random.choice(keys, n_reconstructions, replace=False)
    batch_of_images = np.array([l_dict[key] for key in random_keys])
    return batch_of_images, random_keys

images, random_keys = get_test_batch(5, loaded_dict)

reconstructed_images, latent_representations = reconstruct(encoder, decoder, images)

def compare_reconstructed(n_reconstructions, reconstructed_images, min_max_dict, random_keys, sr, dict_used):
    plt.figure(figsize=(15, n_reconstructions * 3))

    for i, image in enumerate(reconstructed_images):
        file_name = random_keys[i]
        base_file_name = os.path.splitext(file_name)[0]

        min_val = min_max_dict[base_file_name]['min']
        max_val = min_max_dict[base_file_name]['max']

        reconstructed_file_path = f"reconstructed_{base_file_name}.wav"
        original_file_path = f"original_{base_file_name}.wav"
        stft_rec = reconstruct_audio(image, reconstructed_file_path, min_val, max_val)
        stft_og = reconstruct_audio(dict_used[file_name], original_file_path, min_val, max_val)

        original_audio, sr = sf.read(os.path.join('samples', original_file_path))
        reconstructed_audio, sr = sf.read(os.path.join('samples', reconstructed_file_path))

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'Analysis for {file_name}')

        ax1.plot(original_audio)
        ax1.set_title('Original Audio')
        ax1.set_xlabel('Samples')
        ax1.set_ylabel('Amplitude')

        ax2.plot(reconstructed_audio)
        ax2.set_title('Reconstructed Audio')
        ax2.set_xlabel('Samples')
        ax2.set_ylabel('Amplitude')

        im1 = ax3.imshow(stft_og, aspect='auto', origin='lower', extent=[0, stft_og.shape[1], 0, sr / 2])
        fig.colorbar(im1, ax=ax3, format='%+2.0f dB')
        ax3.set_title('STFT Original')
        ax3.set_xlabel('Time (Frames)')
        ax3.set_ylabel('Frequency (Hz)')

        im2 = ax4.imshow(stft_rec, aspect='auto', origin='lower', extent=[0, stft_rec.shape[1], 0, sr / 2])
        fig.colorbar(im2, ax=ax4, format='%+2.0f dB')
        ax4.set_title('STFT Reconstructed')
        ax4.set_xlabel('Time (Frames)')
        ax4.set_ylabel('Frequency (Hz)')

        for ax in fig.get_axes():
            ax.label_outer()

        plt.tight_layout(rect=[0, 0, 1, 0.95])
        plt.show()

min_max_dict = np.load('min_max_values.npy', allow_pickle=True).item()

compare_reconstructed(5, reconstructed_images, min_max_dict, random_keys, 22050, loaded_dict)

import librosa
import soundfile as sf
import numpy as np
import matplotlib.pyplot as plt

def make_louder(audio_path):
      audio_data, sample_rate = librosa.load(audio_path, sr=None)

      current_max = np.max(np.abs(audio_data))

      desired_max = 0.9

      if current_max == 0:
          amplification_factor = 1
      else:
          amplification_factor = desired_max / current_max

      amplified_audio = audio_data * amplification_factor

      # Ensure the amplified audio does not exceed [-1, 1]
      amplified_audio = np.clip(amplified_audio, -1, 1)

      sf.write('untitled.wav', amplified_audio, sample_rate)

make_louder('samples/reconstructed_0_lucas_31.wav')

save_autoencoder_weights(autoencoder, 'model')

loaded_dict_moni = get_dict("/content/drive/My Drive/AudioGeneration/spectrograms_moni")

images, random_keys = get_test_batch(5, loaded_dict_moni)
reconstructed_images, latent_representations = reconstruct(encoder, decoder, images)
min_max_dict_moni = np.load('min_max_values_moni.npy', allow_pickle=True).item()
compare_reconstructed(5, reconstructed_images, min_max_dict_moni, random_keys, 22050, loaded_dict_moni)

def generate(decoder, num_samples, latent_dim=128):
    z_sample = np.random.normal(size=(num_samples, latent_dim))
    generated_data = decoder.predict(z_sample)

    return generated_data

def graph_from_audio(audio_path):
      signal, sr = sf.read(audio_path)

      D = librosa.stft(signal)
      S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)

      plt.figure(figsize=(12, 8))

      plt.subplot(2, 1, 1)
      plt.plot(signal)
      plt.title("Generated Audio Signal")
      plt.xlabel("Samples")
      plt.ylabel("Amplitude")

      plt.subplot(2, 1, 2)
      librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='log')
      plt.colorbar(format='%+2.0f dB')
      plt.title("Spectrogram of Generated Audio")
      plt.xlabel("Time")
      plt.ylabel("Frequency")

      plt.tight_layout()
      plt.show()

generated_data = generate(decoder, 1)
reconstruct_audio(generated_data[0], "generated.wav", -80, 0)
graph_from_audio("samples/generated.wav")



latent_representations = encoder_mu.predict(train_d)

sorted_filenames = sorted(loaded_dict.keys())
sorted_data = [loaded_dict[filename] for filename in sorted_filenames]
sorted_labels = [int(filename.split('_')[0]) for filename in sorted_filenames]

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

latent_representations = encoder_mu.predict(np.array(sorted_data))

pca = PCA(n_components=2)
principal_components = pca.fit_transform(latent_representations)

plt.figure(figsize=(10, 8))
scatter = plt.scatter(principal_components[:, 0], principal_components[:, 1], c=sorted_labels, cmap='viridis', alpha=0.5)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of Latent Space by Spoken Number')
plt.colorbar(scatter, label='Spoken Number')
plt.show()

from sklearn.manifold import TSNE

# Apply t-SNE to latent representations
tsne = TSNE(n_components=2, perplexity=30, n_iter=300)
tsne_components = tsne.fit_transform(latent_representations)

# Plotting with t-SNE
plt.figure(figsize=(10, 8))
scatter = plt.scatter(tsne_components[:, 0], tsne_components[:, 1], c=sorted_labels, cmap='viridis', alpha=0.5)
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.title('t-SNE of Latent Space by Spoken Number')
plt.colorbar(scatter, label='Spoken Number')
plt.show()

def find_class_mean_latent_vectors(encoder, data, labels, num_classes):
    latent_vectors = encoder.predict(data)
    class_mean_vectors = {}
    for i in range(num_classes):
        class_latent_vectors = latent_vectors[labels == i]
        class_mean_vector = np.mean(class_latent_vectors, axis=0)
        class_mean_vectors[i] = class_mean_vector
    return class_mean_vectors

def generate_from_class(decoder, class_mean_vector, num_samples=1, scale=0.5):
    z_sample = np.random.normal(loc=class_mean_vector, scale=scale, size=(num_samples, class_mean_vector.shape[0]))
    generated_data = decoder.predict(z_sample)
    return generated_data


sorted_data_array = np.array(sorted_data)
sorted_data_reshaped = np.array(sorted_data)
labels_array = np.array(sorted_labels)
class_mean_vectors = find_class_mean_latent_vectors(encoder, sorted_data_reshaped, labels_array, num_classes=10)

chosen_number = 0
generated_audio = generate_from_class(decoder, class_mean_vectors[chosen_number])

reconstruct_audio(generated_audio[0], f"generated_{chosen_number}.wav", -80, 0)
graph_from_audio(f"samples/generated_{chosen_number}.wav")