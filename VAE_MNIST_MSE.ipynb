{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54dd66a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mayal\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mayal\\anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\mayal\\anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import (Dense, Flatten, Reshape, Concatenate, Conv2D,\n",
    "                                     UpSampling2D, BatchNormalization, MaxPooling2D, Conv2DTranspose)\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfpl = tfp.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "331a0286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_point_from_normal_distribution(args):\n",
    "    mu, log_variance = args\n",
    "    epsilon = K.random_normal(shape=K.shape(mu), mean=0., stddev=1.)\n",
    "    return mu + K.exp(log_variance / 2) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "983092a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(y_target, y_predicted, mu, log_variance):\n",
    "    reconstruction_loss = K.mean(K.square(y_target - y_predicted), axis=[1, 2, 3])\n",
    "    kl_loss = -0.5 * K.sum(1 + log_variance - K.square(mu) - K.exp(log_variance), axis=1)\n",
    "    combined_loss = reconstruction_loss * 1.0 + kl_loss\n",
    "    return combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e24b1bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization, Flatten, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def get_encoder(latent_dim):\n",
    "    encoder_input = Input(shape=(256, 64, 1), name='encoder_input')\n",
    "    \n",
    "    # First convolutional block\n",
    "    x = Conv2D(filters=512, kernel_size=3, strides=2, padding='same')(encoder_input)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Second convolutional block\n",
    "    x = Conv2D(filters=256, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Third convolutional block\n",
    "    x = Conv2D(filters=128, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Fourth convolutional block\n",
    "    x = Conv2D(filters=64, kernel_size=3, strides=2, padding='same')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Fifth convolutional block\n",
    "    x = Conv2D(filters=32, kernel_size=3, strides=(2,1), padding='same')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Flatten data and add bottleneck with Gaussian sampling\n",
    "    x = Flatten()(x)\n",
    "    mu = Dense(latent_dim, name=\"mu\")(x)\n",
    "    log_variance = Dense(latent_dim, name=\"log_variance\")(x)\n",
    "    \n",
    "    # Lambda layer for sampling using reparameterization trick\n",
    "    encoder_output = Lambda(sample_point_from_normal_distribution, name=\"encoder_output\")([mu, log_variance])\n",
    "    \n",
    "    # Create model\n",
    "    encoder = Model(encoder_input, [encoder_output, mu, log_variance], name=\"encoder\")\n",
    "    \n",
    "    return encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b0deeb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(latent_dim):\n",
    "    # Define the input\n",
    "    encoder_input = Input(shape=(256, 64, 1), name='encoder_input')\n",
    "\n",
    "    # Convolutional layers\n",
    "    x = Conv2D(filters=512, kernel_size=3, strides=2, padding='same')(encoder_input)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Continue with the rest of the blocks\n",
    "    ...\n",
    "    \n",
    "    # Flatten data and create bottleneck with Gaussian sampling\n",
    "    x = Flatten()(x)\n",
    "    mu = Dense(latent_dim, name=\"mu\")(x)\n",
    "    log_variance = Dense(latent_dim, name=\"log_variance\")(x)\n",
    "\n",
    "    # Lambda layer for reparameterization trick\n",
    "    encoder_output = Lambda(sample_point_from_normal_distribution, name=\"encoder_output\")([mu, log_variance])\n",
    "    \n",
    "    # Return encoder with the output tensors\n",
    "    encoder = Model(encoder_input, [encoder_output, mu, log_variance], name=\"encoder\")\n",
    "    \n",
    "    return encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "75e1a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_combined_loss(y_target, y_predicted, mu, log_variance, reconstruction_loss_weight):\n",
    "    reconstruction_loss = K.mean(K.square(y_target - y_predicted), axis=[1, 2, 3])\n",
    "    kl_loss = -0.5 * K.sum(1 + log_variance - K.square(mu) - K.exp(log_variance), axis=1)\n",
    "    combined_loss = reconstruction_loss_weight * reconstruction_loss + kl_loss\n",
    "    return combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "49d25940",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[2097152,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:AddV2] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0005\u001b[39m \n\u001b[0;32m      3\u001b[0m reconstruction_loss_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000000\u001b[39m\n\u001b[1;32m----> 5\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mget_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m decoder \u001b[38;5;241m=\u001b[39m get_decoder(latent_dim)\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;66;03m# VAE architecture\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[62], line 15\u001b[0m, in \u001b[0;36mget_encoder\u001b[1;34m(latent_dim)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Flatten data and create bottleneck with Gaussian sampling\u001b[39;00m\n\u001b[0;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m Flatten()(x)\n\u001b[1;32m---> 15\u001b[0m mu \u001b[38;5;241m=\u001b[39m \u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m log_variance \u001b[38;5;241m=\u001b[39m Dense(latent_dim, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_variance\u001b[39m\u001b[38;5;124m\"\u001b[39m)(x)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Lambda layer for reparameterization trick\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend.py:2100\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[1;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[0;32m   2098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonce:\n\u001b[0;32m   2099\u001b[0m         seed \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mstateless_fold_in(seed, nonce)\n\u001b[1;32m-> 2100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless_uniform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\n\u001b[0;32m   2108\u001b[0m     shape\u001b[38;5;241m=\u001b[39mshape,\n\u001b[0;32m   2109\u001b[0m     minval\u001b[38;5;241m=\u001b[39mminval,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2112\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_legacy_seed(),\n\u001b[0;32m   2113\u001b[0m )\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[2097152,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:AddV2] name: "
     ]
    }
   ],
   "source": [
    "latent_dim = 128\n",
    "learning_rate = 0.0005 \n",
    "reconstruction_loss_weight = 1000000\n",
    "\n",
    "encoder = get_encoder(latent_dim)\n",
    "decoder = get_decoder(latent_dim)\n",
    "\n",
    "        # VAE architecture\n",
    "vae_input = Input(shape=(256, 64, 1), name='vae_input')\n",
    "latent_representation = encoder(vae_input)\n",
    "decoded_output = decoder(latent_representation)\n",
    "\n",
    "        # Custom loss calculation in a lambda layer to extract mu and log_variance\n",
    "mu = encoder.get_layer('mu').output\n",
    "log_variance = encoder.get_layer('log_variance').output\n",
    "\n",
    "        # Create VAE model\n",
    "vae = Model(vae_input, decoded_output, name='vae')\n",
    "\n",
    "        # Compile with custom loss\n",
    "vae.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss=lambda y_true, y_pred: calculate_combined_loss(y_true, y_pred, mu, log_variance, reconstruction_loss_weight),\n",
    "    metrics=['mse']\n",
    ")\n",
    "\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6886d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "directory_path = \"spectrograms\"\n",
    "\n",
    "loaded_dict = {}\n",
    "\n",
    "file_list = os.listdir(directory_path)\n",
    "\n",
    "for file_name in file_list:\n",
    "    if file_name.endswith('.npy'):  \n",
    "        file_path = os.path.join(directory_path, file_name)  \n",
    "        array = np.load(file_path)  \n",
    "        array_with_new_axis = np.expand_dims(array, axis=-1)\n",
    "        loaded_dict[file_name] = array_with_new_axis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6addf293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\mayal\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\mayal\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\mayal\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\mayal\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\mayal\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\mayal\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'vae' (type Functional).\n    \n    Layer \"decoder\" expects 1 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor 'vae/encoder/encoder_output/add:0' shape=(None, 128) dtype=float32>, <tf.Tensor 'vae/encoder/mu/BiasAdd:0' shape=(None, 128) dtype=float32>, <tf.Tensor 'vae/encoder/log_variance/BiasAdd:0' shape=(None, 128) dtype=float32>]\n    \n    Call arguments received by layer 'vae' (type Functional):\n      • inputs=tf.Tensor(shape=(None, None, None, None), dtype=float32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m train_gen \u001b[38;5;241m=\u001b[39m data_generator(loaded_dict, batch_size)\n\u001b[0;32m     25\u001b[0m train_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(loaded_dict) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size\n\u001b[1;32m---> 26\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_cb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filef8aa0tfr.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\mayal\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\mayal\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\mayal\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\mayal\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\mayal\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\mayal\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'vae' (type Functional).\n    \n    Layer \"decoder\" expects 1 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor 'vae/encoder/encoder_output/add:0' shape=(None, 128) dtype=float32>, <tf.Tensor 'vae/encoder/mu/BiasAdd:0' shape=(None, 128) dtype=float32>, <tf.Tensor 'vae/encoder/log_variance/BiasAdd:0' shape=(None, 128) dtype=float32>]\n    \n    Call arguments received by layer 'vae' (type Functional):\n      • inputs=tf.Tensor(shape=(None, None, None, None), dtype=float32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    'best_model_MNIST.keras', \n",
    "    save_best_only=True,\n",
    "    monitor='val_loss', \n",
    "    mode='min' \n",
    ")\n",
    "\n",
    "def data_generator(data_dict, batch_size):\n",
    "    keys = list(data_dict.keys())  \n",
    "    while True:\n",
    "        np.random.shuffle(keys)  \n",
    "        for i in range(0, len(keys), batch_size):\n",
    "            key_batch = keys[i:i + batch_size]\n",
    "            x_batch = np.array([data_dict[key] for key in key_batch])\n",
    "            yield x_batch, x_batch \n",
    "            \n",
    "batch_size = 16\n",
    "train_gen = data_generator(loaded_dict, batch_size)\n",
    "\n",
    "train_steps = len(loaded_dict) // batch_size\n",
    "history = vae.fit(\n",
    "    x=train_gen,  \n",
    "    steps_per_epoch=train_steps,  \n",
    "    epochs=15,  \n",
    "    callbacks=[checkpoint_cb]  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac2f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import tensorflow as tf \n",
    "\n",
    "def inverse_normalize_spectrogram(normalized_spectrogram, min_val, max_val):\n",
    "    return normalized_spectrogram * (max_val - min_val) + min_val\n",
    "\n",
    "def reconstruct_audio(stft, output_path, min_val, max_val):\n",
    "    stft = stft[:, :, 0]\n",
    "    magnitude_db = inverse_normalize_spectrogram(stft, min_val, max_val)\n",
    "    magnitude_linear = librosa.db_to_amplitude(magnitude_db)\n",
    "    y_reconstructed = librosa.istft(magnitude_linear, hop_length=256)\n",
    "    sf.write(output_path, y_reconstructed, 22050)\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(y_reconstructed, n_fft=512, hop_length=256)), ref=np.max)\n",
    "    librosa.display.specshow(D, sr=22050, hop_length=256, x_axis='time', y_axis='linear')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Reconstructed Spectrogram')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc76fd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_music(prior, decoder, n_samples):\n",
    "    z = prior.sample(n_samples)\n",
    "    return decoder(z).mean()\n",
    "\n",
    "n_samples = 5\n",
    "sm = generate_music(prior, decoder, n_samples)\n",
    "reconstruct_audio(sm[1], \"originalu.wav\", -80, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2824d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "\n",
    "def reconstruct(encoder, decoder, batch_of_images):\n",
    "    latent_representations = encoder(batch_of_images)  \n",
    "    distribution = decoder(latent_representations)  \n",
    "    reconstructed_image_tensor = distribution.mean()  \n",
    "    return reconstructed_image_tensor\n",
    "\n",
    "n_reconstructions = 5\n",
    "keys = list(loaded_dict.keys())\n",
    "random_keys = np.random.choice(keys, n_reconstructions, replace=False)\n",
    "batch_of_images = np.array([loaded_dict[key] for key in random_keys])\n",
    "reconstructions = reconstruct(encoder, decoder, batch_of_images)\n",
    "reconstructed_images_np = reconstructions.numpy()\n",
    "\n",
    "def compare_reconstructed(reconstructed_images_np, min_max_dict, random_keys):\n",
    "    for i, image in enumerate(reconstructed_images_np):\n",
    "        file_name = random_keys[i]\n",
    "        base_file_name = os.path.splitext(file_name)[0]\n",
    "\n",
    "        min_val = min_max_dict[base_file_name]['min']\n",
    "        max_val = min_max_dict[base_file_name]['max']\n",
    "\n",
    "        reconstruct_audio(image, f\"reconstructed_{file_name}.wav\", min_val, max_val)\n",
    "        reconstruct_audio(loaded_dict[file_name], f\"original_{file_name}.wav\", min_val, max_val)\n",
    "\n",
    "min_max_dict = np.load('min_max_values.npy', allow_pickle=True).item()\n",
    "compare_reconstructed(reconstructed_images_np, min_max_dict, random_keys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
